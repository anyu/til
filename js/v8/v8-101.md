# V8

## Overview

JS execution in V8 has 3 main stages:

1. **Source to syntax tree**: Parser generates an AST from the source
2. **Syntax tree to bytecode**: V8's interpreter, Ignition, generates bytecode from the syntax tree (note: only since post-2017)
3. **Bytecode to machine code**: V8's compiler, TurboFan, generates a graph from bytecode, replacing sections of it with optimized machine code

In general, there are 2 ways to translate source code to machine code:

## Using an **interpreter**
Quick to get started, slow to execute

- Uses a REPL (read-eval-print loop) to translate/execute line-by-line
- Pros: easier to implement, easier to debug since errors are immediate
- Cons: eval has overhead compared to running machine code, hard to optimize across parts of the program (eg. can't recognize dupe effort)

## Using a **compiler**
Complex to get started, fast to execute
- Scans entire source code and translates to machine code before execution
- Produces an output file that can be run independently
- Pros: can make global optimizations (share machine code for dupe code)
- Cons: can only catch syntax and semantic errors, requires more memory

## Just-in-time (JIT) compilation

Combines best of both.

1. Profiler runs code through an interpreter and tracks warm code segments (code that runs a few times) and hot code segments (code that runs many times)
2. JIT sends warm code segments to a baseline compiler, reusing compiled code where possible.
3. JIT sends hot warm code segments to an optimizing compiler, which uses info from the interpreter to make assumptions & optimizations. If assumptions are wrong, the optimizing compiler performs **deoptimization** (discards optimized code). Optimization and deoptimization cycles are expensive.

Cons:
- adds overhead memory costs due to storing optimized machine code and profiler information

## V8 compilation
V8's interpreter Ignition and compiler TurboFan do the following:
- Ignition translates AST to bytecode. Bytecode is executed and feedback is collected via inline caches and provided to Ignition for subsequent interpretation and to TurboFan for speculative optimization.
- Turbofan speculatively optimizes bytecode by translating it into architecture-specific machine code based on that feedback.

### Ignition
- Ignition addresses JIT's memory consumption overhead and looks to reduce memory usage, startup time, and complexity.
- Does this by compiling AST to bytecode and collecting feedback during execution
    - Bytecode becomes source of truth, removing need to re-parse JS during compilation. Deoptimization process no longer needs original source code.
    - Inline cache lets V8 optimized repeated calls to a function w/ the same type args. The inline cache stores the types of inputs to a function, so the fewer types, the fewer type checks are needed.